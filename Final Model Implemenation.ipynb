{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import lightgbm as lgbm\n",
    "import warnings\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"optiver_raw_data/train.csv\")\n",
    "time_id_of_first_80_percent = train.iloc[18384,1]\n",
    "\n",
    "stock_0 = train[train[\"stock_id\"]==0]\n",
    "df_train = pd.DataFrame()\n",
    "df_test = pd.DataFrame()\n",
    "for i in range(127):\n",
    "    stock_i = train[train[\"stock_id\"]==i]\n",
    "\n",
    "    stock_train = stock_i[stock_i['time_id'] <= time_id_of_first_80_percent]\n",
    "    stock_test = stock_i[stock_i['time_id'] > time_id_of_first_80_percent]\n",
    "\n",
    "    df_train = pd.concat([df_train, stock_train])\n",
    "    df_test = pd.concat([df_test, stock_test])\n",
    "\n",
    "df_train.to_parquet('target_data/target_train.parquet')\n",
    "df_test.to_parquet('target_data/target_test.parquet')\n",
    "\n",
    "\n",
    "stock_0 = train[train[\"stock_id\"]==0]\n",
    "df_train = pd.DataFrame()\n",
    "df_test = pd.DataFrame()\n",
    "for i in range(127):\n",
    "    stock_i = train[train[\"stock_id\"]==i]\n",
    "\n",
    "    stock_train = stock_i[stock_i['time_id'] <= time_id_of_first_80_percent]\n",
    "    stock_test = stock_i[stock_i['time_id'] > time_id_of_first_80_percent]\n",
    "\n",
    "    df_train = pd.concat([df_train, stock_train])\n",
    "    df_test = pd.concat([df_test, stock_test])\n",
    "\n",
    "df_train.to_parquet('target_data/target_train.parquet')\n",
    "df_test.to_parquet('target_data/target_test.parquet')\n",
    "\n",
    "for i in range(127):\n",
    "    filename = \"optiver_raw_data/trade_train.parquet/stock_id=\" + str(i)\n",
    "    if not os.path.exists(filename):\n",
    "        continue\n",
    "    trade_current_stock = pd.read_parquet(filename)\n",
    "\n",
    "    stock_train = trade_current_stock[trade_current_stock['time_id'] <= time_id_of_first_80_percent]\n",
    "    stock_train.to_parquet('stock_trade_train/stock_' + str(i) + '_train.parquet')\n",
    "\n",
    "    stock_test = trade_current_stock[trade_current_stock['time_id'] > time_id_of_first_80_percent]\n",
    "    stock_test.to_parquet('stock_trade_test/stock_' + str(i) + '_test.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wap(df):\n",
    "        return (df['bid_price1'] * df['ask_size1'] +\n",
    "                df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "\n",
    "def wap2(df):\n",
    "    return (df['bid_price2'] * df['ask_size2'] +\n",
    "            df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "\n",
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "def realized_volatility(returns):\n",
    "    return np.sqrt(np.sum(returns ** 2))\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "def isValidStock(i):\n",
    "    filename = \"stock_trade_train/stock_\" + str(i) + \"_train.parquet\"\n",
    "    if not os.path.exists(filename):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def RMSPEMetric():\n",
    "\n",
    "    def RMSPE(y_hat, dtrain):\n",
    "        y = dtrain.get_label()\n",
    "        elements = ((y - y_hat) / y) ** 2\n",
    "        return 'RMSPE', float(np.sqrt(np.sum(elements) / len(y))), False\n",
    "\n",
    "    return RMSPE\n",
    "\n",
    "def RMSPE(y_hat, y):\n",
    "    elements = ((y - y_hat) / y) ** 2\n",
    "    return float(np.sqrt(np.sum(elements) / len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_predictors(stock_id, train_or_test):\n",
    "    stock_data = pd.read_parquet('stock_book_' + train_or_test + '/stock_' + str(stock_id) + '_' + train_or_test + '.parquet')\n",
    "\n",
    "    # Only consider last 5 mins of data for all book predictors\n",
    "    # stock_data = stock_data[stock_data[\"seconds_in_bucket\"] >= 300] #Only consider seconds_in_bucket > 300 \n",
    "\n",
    "    stock_data['wap'] = wap(stock_data)\n",
    "    stock_data['log_return'] = stock_data.groupby('time_id')['wap'].apply(log_return)\n",
    "    stock_data['wap2'] = wap2(stock_data)\n",
    "    stock_data['log_return2'] = stock_data.groupby('time_id')['wap2'].apply(log_return)\n",
    "    stock_data['wap_offset'] = abs(stock_data['wap'] - stock_data['wap2'])\n",
    "    stock_data['price_spread'] = (stock_data['ask_price1'] - stock_data['bid_price1']) / ((stock_data['ask_price1'] + stock_data['bid_price1']) / 2)\n",
    "    stock_data['bid_spread'] = stock_data['bid_price1'] - stock_data['bid_price2']\n",
    "    stock_data['ask_spread'] = stock_data['ask_price1'] - stock_data['ask_price2']\n",
    "    stock_data['total_volume'] = (stock_data['ask_size1'] + stock_data['ask_size2']) + (stock_data['bid_size1'] + stock_data['bid_size2'])\n",
    "    stock_data['volume_imbalance'] = abs((stock_data['ask_size1'] + stock_data['ask_size2']) - (stock_data['bid_size1'] + stock_data['bid_size2']))\n",
    "\n",
    "    #Adding features \n",
    "\n",
    "\n",
    "    create_feature_dict = {\n",
    "            'log_return':[realized_volatility],\n",
    "            'log_return2':[realized_volatility],\n",
    "            'wap_offset':[np.mean],\n",
    "            'price_spread':[np.mean],\n",
    "            'bid_spread':[np.mean],\n",
    "            'ask_spread':[np.mean],\n",
    "            'volume_imbalance':[np.mean],\n",
    "            'total_volume':[np.mean],\n",
    "            'wap':[np.mean],\n",
    "    }\n",
    "\n",
    "    result = pd.DataFrame(stock_data.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n",
    "    result.columns = result.columns.map('_'.join).str.strip('_')\n",
    "\n",
    "    return result\n",
    "\n",
    "def trade_predictors(stock_id, train_or_test):\n",
    "    stock_data = pd.read_parquet('stock_trade_' + train_or_test + '/stock_' + str(stock_id) + '_' + train_or_test + '.parquet')\n",
    "\n",
    "    stock_data['log_return'] = stock_data.groupby('time_id')['price'].apply(log_return)\n",
    "\n",
    "    stock_data['price_max'] = stock_data[\"price\"]\n",
    "    stock_data['price_min'] = stock_data[\"price\"]\n",
    "    stock_data['price_median'] = stock_data[\"price\"]\n",
    "\n",
    "    aggregate_dictionary = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum], \n",
    "        'price_max':[np.max],\n",
    "        'price_min':[np.min],\n",
    "        'price_median':[np.median],\n",
    "    }\n",
    "    result = pd.DataFrame(stock_data.groupby('time_id').agg(aggregate_dictionary)).reset_index()\n",
    "    result[\"size_per_order\"] = result[\"size\"] / result[\"order_count\"] \n",
    "\n",
    "    avg_trade_volume = result[\"size\"].mean()\n",
    "    result[\"rel_trade_volume\"] = result[\"size\"] / avg_trade_volume\n",
    "    result['current_percent_range'] = (result[\"price_max\"].values - result[\"price_min\"].values) / result[\"price_median\"]\n",
    "    avg_percent_range = result['current_percent_range'].mean()\n",
    "    result['rel_percent_range'] = result['current_percent_range'] /  avg_percent_range\n",
    "    result['stock_id'] = stock_id\n",
    "\n",
    "    result.columns = result.columns.map('_'.join).str.strip('_')\n",
    "    return result\n",
    "\n",
    "def target(stock_id, train_or_test):\n",
    "    result = pd.read_parquet('target_data/target_' + train_or_test + '.parquet')\n",
    "    result = result.loc[result['stock_id'] == stock_id]\n",
    "    result = result.drop(['stock_id'], axis = 1)\n",
    "    return result\n",
    "\n",
    "def generate_data(stock_id, train_or_test):\n",
    "    result = pd.merge(target(stock_id, train_or_test), book_predictors(stock_id, train_or_test), on='time_id', how='left')\n",
    "    result = pd.merge(result, trade_predictors(stock_id, train_or_test), on='time_id', how='left')\n",
    "    result = result.dropna()\n",
    "    return result\n",
    "\n",
    "def generate_train_and_test(stock_id):\n",
    "    train = generate_data(stock_id, 'train')\n",
    "    test = generate_data(stock_id, 'test')\n",
    "\n",
    "    X_train = train.drop(['target', 'time_id'], axis = 1)\n",
    "    X_test = test.drop(['target', 'time_id'], axis = 1)\n",
    "\n",
    "    y_train = train['target']\n",
    "    y_test = test['target']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting combined Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "total_size = 0\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_train = pd.DataFrame()\n",
    "X_test = pd.DataFrame()\n",
    "y_train = pd.DataFrame()\n",
    "y_test = pd.DataFrame()\n",
    "\n",
    "for i in range(127):\n",
    "    if not isValidStock(i):\n",
    "        continue\n",
    "\n",
    "    a, b, c, d = generate_train_and_test(i)\n",
    "\n",
    "    X_train = pd.concat([X_train, a])\n",
    "    X_test = pd.concat([X_test, b])\n",
    "    y_train = pd.concat([y_train, c])\n",
    "    y_test = pd.concat([y_test, d])\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgbm = {\n",
    "        'boosting_type': 'goss',\n",
    "        'learning_rate': 0.001,\n",
    "        'objective': 'rmse',\n",
    "        'n_jobs': 8,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'verbose': -1,\n",
    "    }\n",
    "weights = 1/(y_train[0] ** 2)\n",
    "lgbm_train_data = lgbm.Dataset(X_train, label=y_train[0], weight=weights)\n",
    "\n",
    "weights = 1/(y_test[0] ** 2)\n",
    "lgbm_test_data = lgbm.Dataset(X_test, label=y_test[0], weight=weights)\n",
    "\n",
    "evals={}\n",
    "rounds = 100000\n",
    "model = lgbm.train(params_lgbm, \n",
    "                    lgbm_train_data, \n",
    "                    rounds, \n",
    "                    valid_sets=lgbm_test_data,\n",
    "                    feval=RMSPEMetric(),\n",
    "                    verbose_eval = 1000,\n",
    "                    early_stopping_rounds=3000,\n",
    "                    categorical_feature = ['stock_id'],\n",
    "                    callbacks = [lgbm.record_evaluation(evals)]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSPE over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "sns.set(font_scale = 2)\n",
    "sns.lineplot(data=evals.get('valid_0').get('l2'))\n",
    "plt.title('RMSPE vs. Iterations')\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSPE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame({'Value':model.feature_importance(),'Feature':X_train.columns})\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.set(font_scale = 2)\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:100])\n",
    "plt.title('LightGBM Feature Importance')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d41f633da6a0ed462606ff387ac8d93faacd97ec55d72fd1ffe37702897b9b90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
