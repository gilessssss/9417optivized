{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import glob \n",
    "import warnings \n",
    "import plotly.express as px \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wap1(row):\n",
    "    denom = row.ask_size1 + row.bid_size1\n",
    "    return ((row.bid_price1 * row.ask_size1 + row.ask_price1 * row.bid_size1)/denom)\n",
    "    \n",
    "def wap2(row):\n",
    "    denom = row.ask_size2 + row.bid_size2\n",
    "    return ((row.bid_price2 * row.ask_size2 + row.ask_price2 * row.bid_size2)/denom)\n",
    "\n",
    "def log_avg_wap(row):\n",
    "    return np.log((row.wap1 + row.wap2)/2)\n",
    "\n",
    "def log_return(list_prices):\n",
    "    return np.log(list_prices).diff()\n",
    "def realized_volatility(series_log_return):\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "def custom_loss(ytrue,ypred) :\n",
    "    squared_residual = (ytrue-ypred)**2/ytrue\n",
    "    grad = squared_residual\n",
    "    hess = np.ones(len(ytrue))\n",
    "    \n",
    "    return grad,hess\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "def feval_RMSPE(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n",
    "def custom_rmspe_valid(y_true, y_pred):\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    residual = residual ** 2 / y_true\n",
    "    residual = np.mean(residual)\n",
    "    return \"eval_RMSPE\", math.sqrt(residual), False\n",
    "def simple_volatility(series_prix):\n",
    "    mx = np.max(series_prix)\n",
    "    mn = np.min(series_prix)\n",
    "    moy = np.mean(series_prix)\n",
    "    vol = (moy-mn)/(mx-mn)\n",
    "    return vol\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_parquet(\"target_data/target_train.parquet\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_train_stock_id_0 = pd.read_parquet(\"stock_book_train/stock_18_train.parquet\")\n",
    "book_train_stock_id_0_t_id5 = book_train_stock_id_0[book_train_stock_id_0[\"time_id\"]==5]\n",
    "#book_train_stock_id_0_t_id5\n",
    "book_train_stock_id_0_t_id5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_train_stock_id_0 = pd.read_parquet(\"stock_trade_train/stock_18_train.parquet\")\n",
    "trade_train_stock_id_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_train_stock_id_0.groupby(['time_id']).mean().reset_index().iloc[1013:1015,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 i) Feature Engineering (TRADE DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature 1 -> Relative volume by 10min bucket\n",
    "\n",
    "#1. Finding average size (volume) across all time_id's\n",
    "\n",
    "avg_trade_volume_stock_id_0 = trade_train_stock_id_0.groupby(['time_id']).sum()[\"size\"].mean() #this groups by time id, gets the total sum of the size, then it gets the mean across all time buckets \n",
    " \n",
    "#2. Get size (volume) of time_id = 5\n",
    "trade_volume_stock_id_0_time_id_5 = trade_train_stock_id_0[trade_train_stock_id_0['time_id'] == 5][\"size\"].sum() #gives me the volume of time bucket 5\n",
    "\n",
    "#3. Compute relative volume\n",
    "rel_trade_volume_stock_id_0_time_id_5 = trade_volume_stock_id_0_time_id_5/avg_trade_volume_stock_id_0\n",
    "rel_trade_volume_stock_id_0_time_id_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_trade_volume_stock_id_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_train_stock_id_0['time_id'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature 2 -> Relative price range \n",
    "\n",
    "#1. Find min and max of trade price for all seperate time_id. \n",
    "min_trade_price_stock_id_0 = trade_train_stock_id_0.groupby(['time_id']).min()\n",
    "max_trade_price_stock_id_0 = trade_train_stock_id_0.groupby(['time_id']).max()\n",
    "#2. Find range of trade price and median of trade price for all seperate time_id. \n",
    "range_trade_price_stock_id_0 = max_trade_price_stock_id_0 - min_trade_price_stock_id_0 \n",
    "median_trade_price_stock_id_0 = trade_train_stock_id_0.groupby(['time_id']).median()\n",
    "#3. Use median to compute how much percent below our minimum is for all seperate time_id. \n",
    "lower_percent_range_relative_to_median = (median_trade_price_stock_id_0 - min_trade_price_stock_id_0)/median_trade_price_stock_id_0\n",
    "#4. Use median to compute how much percent above our maximum is for all seperate time_id. \n",
    "upper_percent_range_relative_to_median = (max_trade_price_stock_id_0 - median_trade_price_stock_id_0)/median_trade_price_stock_id_0\n",
    "#5 Add both values to get total percent range. E.g. 3% below median and 5% above median = 8% total range for all seperate time_id.\n",
    "total_percent_range = upper_percent_range_relative_to_median + lower_percent_range_relative_to_median\n",
    "\n",
    "#6. Compute the average percent range across all time id's\n",
    "\n",
    "avg_total_percent_range = total_percent_range[\"price\"].mean()\n",
    "\n",
    "#7. Get total percent range for time id 5\n",
    "min_price_stock_id_0_time_id_5 = trade_train_stock_id_0[trade_train_stock_id_0['time_id'] == 5][\"price\"].min()\n",
    "max_price_stock_id_0_time_id_5 = trade_train_stock_id_0[trade_train_stock_id_0['time_id'] == 5][\"price\"].max()\n",
    "\n",
    "range_stock_id_0_time_id_5 = max_price_stock_id_0_time_id_5 - min_price_stock_id_0_time_id_5 #gives me the range of price for time bucket 5\n",
    "median_stock_id_0_time_id_5 = trade_train_stock_id_0[trade_train_stock_id_0['time_id'] == 5][\"price\"].median()\n",
    "\n",
    "lower_percent_range_relative_to_median_stock_id_0_time_id_5 = (median_stock_id_0_time_id_5 - min_price_stock_id_0_time_id_5)/median_stock_id_0_time_id_5\n",
    "\n",
    "upper_percent_range_relative_to_median_stock_id_0_time_id_5= (max_price_stock_id_0_time_id_5 - median_stock_id_0_time_id_5)/median_stock_id_0_time_id_5\n",
    "\n",
    "total_percent_range_stock_id_0_time_id_5 = lower_percent_range_relative_to_median_stock_id_0_time_id_5 + upper_percent_range_relative_to_median_stock_id_0_time_id_5\n",
    "\n",
    "#8. Compute relative percent trading range to average\n",
    "rel_total_percent_range_stock_id_0_time_id_5 = total_percent_range_stock_id_0_time_id_5/avg_total_percent_range\n",
    "rel_total_percent_range_stock_id_0_time_id_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_percent_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_total_percent_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_percent_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature 3 -> size_per_order\n",
    "size = \n",
    "size_per_order = trade_train_stock_id_0.groupby(['time_id']).sum()['order_count']\n",
    "order_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature 3 -> Time between execution --> size/second "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 ii) Feature Engineering (BOOK DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Feature Engineering for ORDER BOOK data a###\n",
    "book_train_stock_id_0.groupby(['time_id']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the first weighted averaged price for each seconds_in_bucket and time ID\n",
    " \n",
    "#book_train_stock_id_0[\"wap1\"] = book_train_stock_id_0.apply(wap1,axis=1)\n",
    "denom1 = book_train_stock_id_0[\"ask_size1\"] + book_train_stock_id_0[\"bid_size1\"]\n",
    "volprice1 = book_train_stock_id_0[\"bid_price1\"] * book_train_stock_id_0[\"ask_size1\"] + book_train_stock_id_0[\"ask_price1\"] * book_train_stock_id_0[\"bid_size1\"]\n",
    "book_train_stock_id_0[\"wap1\"] = volprice1/denom1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the second weighted averaged price for each seconds_in_bucket and time ID \n",
    "\n",
    "#book_train_stock_id_0.loc[:, \"wap2\"] = book_train_stock_id_0.apply(wap2,axis=1)\n",
    "denom2 = book_train_stock_id_0[\"ask_size2\"] + book_train_stock_id_0[\"bid_size2\"]\n",
    "volprice2 = book_train_stock_id_0[\"bid_price2\"] * book_train_stock_id_0[\"ask_size2\"] + book_train_stock_id_0[\"ask_price2\"] * book_train_stock_id_0[\"bid_size2\"]\n",
    "book_train_stock_id_0[\"wap2\"] = volprice2/denom2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the avg weighted price using both wap1 and wap2 for each seconds_in_bucket and time ID \n",
    "\n",
    "#book_train_stock_id_0.loc[:, \"log_avg_wap\"] = book_train_stock_id_0.apply(log_avg_wap,axis=1)\n",
    "book_train_stock_id_0[\"avg_wap\"] = (book_train_stock_id_0[\"wap1\"] + book_train_stock_id_0[\"wap2\"])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theory, the bigger the spread the higher the volatility!!!\n",
    "#Getting spread ratio's of 1 and 2\n",
    "spread_ratio_1 = book_train_stock_id_0[\"ask_price1\"]/book_train_stock_id_0[\"bid_price1\"]\n",
    "book_train_stock_id_0[\"spread_ratio_1\"] = spread_ratio_1\n",
    "spread_ratio_2 = book_train_stock_id_0[\"ask_price2\"]/book_train_stock_id_0[\"bid_price2\"]\n",
    "book_train_stock_id_0[\"spread_ratio_2\"] = spread_ratio_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute volume imbalance as an average ratio (supply/demand) per time_id\n",
    "total_bid_size = book_train_stock_id_0[\"bid_size1\"] + book_train_stock_id_0[\"bid_size2\"]\n",
    "total_ask_size = book_train_stock_id_0[\"ask_size1\"] + book_train_stock_id_0[\"ask_size2\"]\n",
    "book_train_stock_id_0[\"vol_imbalance\"] = total_ask_size/total_bid_size\n",
    "#Finding the average volume imbalance by time_id\n",
    "vol_imbalance = book_train_stock_id_0.groupby(['time_id']).mean()[\"vol_imbalance\"]\n",
    "book_train_stock_id_0.groupby(['time_id']).mean()\n",
    "\n",
    "#how much it deviates from a ratio of 1:1 means its more significant a ratio of 10 is the same significance as a ratio of 0.1? So maybe we standardize it and say if ratio <1 do 1/ratio... so ratios like 0.1 can be changed to 10!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Exploration (e.g. correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring previous realized vol against target next 10 min for stock id 0\n",
    "target_train = pd.read_parquet(\"target_data/target_train.parquet\")\n",
    "target_train_stock_id_0 = target_train[target_train[\"stock_id\"] == 0]\n",
    "target_train_stock_id_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the log returns for stock 0 using avg_wap\n",
    "book_train_stock_id_0.loc[:, 'log_return'] = log_return(book_train_stock_id_0[\"avg_wap\"])\n",
    "#just extract all rows that aren't null...\n",
    "book_train_stock_id_0 = book_train_stock_id_0[~book_train_stock_id_0['log_return'].isnull()]\n",
    "#examine time ID 5 by itself\n",
    "book_train_stock_id_0_time_id_5 = book_train_stock_id_0[book_train_stock_id_0['time_id']==5]\n",
    "book_train_stock_id_0_time_id_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting log returns for stock 0\n",
    "fig = px.line(book_train_stock_id_0_time_id_5, x=\"seconds_in_bucket\", y=\"log_return\", title='Log return of stock_id_0, time_id_5')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating realized volatility for stock0, time_id 5 \n",
    "realized_vol = realized_volatility(book_train_stock_id_0_time_id_5['log_return'])\n",
    "realized_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is previous 10min volatility related to next 10 min vol???!!!\n",
    "realized_vol_id_0 = book_train_stock_id_0.groupby(\"time_id\")[\"log_return\"].apply(realized_volatility) #going through book data of stock 0, grouping by time_id and getting realized volatility using log returns\n",
    "realized_vol_id_0\n",
    "vol_id_0_df = pd.DataFrame(columns=[target_train_stock_id_0[\"target\"], realized_vol_id_0])\n",
    "realized_vol_id_0\n",
    "#target_train_stock_id_0[\"target\"]\n",
    "vol_id_0_df = vol_id_0_df.T\n",
    "vol_id_0_df\n",
    "vol_id_0_df.reset_index(inplace=True)\n",
    "vol_id_0_df = vol_id_0_df.rename(columns={\"target\": \"Target Volatility\", \"log_return\": \"Realized Volatility\"})\n",
    "vol_id_0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(vol_id_0_df, x='Realized Volatility', y='Target Volatility', title='Target Volatility vs Realized Volatility Last 10 min')\n",
    "fig.update_yaxes(nticks=20)\n",
    "fig.update_xaxes(nticks=20)\n",
    "fig.update_layout(xaxis_range=[0, 0.03])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_id_0_df.corr()\n",
    "#It appears that we have a strong positive correlation between realized volatility and target volatility of the prev 10 min? Is this random? Lets check the other 10 min intervals!!!!!\n",
    "#WAP_AVG to calculate realized vol of first 10 mins\n",
    "#WAP1 might be better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conduct a quick random algorithm to show that only the current time_id's was related to the next 10 mins (i.e. target volatility), and previous time_id's had no effect on later time_id's... \n",
    "\n",
    "#Use sample function to shift around target volatility data randomly\n",
    "vol_id_0_df_shifted = vol_id_0_df.copy()\n",
    "vol_id_0_df_shifted[\"Target Volatility\"] = vol_id_0_df_shifted[\"Target Volatility\"].sample(frac=1).reset_index(drop=True)\n",
    "#np.random.uniform(low=0, high=)\n",
    "vol_id_0_df_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global test\n",
    "global correlation_dict\n",
    "correlation_dict = {\n",
    "    \"stock_id\": [],\n",
    "    \"realized_t_corr\": [],\n",
    "    \"realized_t_1_corr\": [],\n",
    "    \"realized_t_2_corr\": [],\n",
    "    \"realized_t_3_corr\": [],\n",
    "    \"realized_t_4_corr\": [],\n",
    "    \"realized_t_5_corr\": [],\n",
    "\n",
    "}\n",
    "def isValidStock(i):\n",
    "    filename = \"stock_trade_train/stock_\" + str(i) + \"_train.parquet\"\n",
    "    print(filename)\n",
    "    if not os.path.exists(filename):\n",
    "        return False\n",
    "    return True\n",
    "global tester \n",
    "def corrHeatmap(subset):\n",
    "    global correlation_dict\n",
    "    for j in range(127):\n",
    "        if not isValidStock(j):\n",
    "            continue\n",
    "\n",
    "        #generalizing an algorithm for all stocks to create average correlation matrix... \n",
    "        #Begin by reading in the target train data for every stock... \n",
    "        parquet_path = \"stock_book_train/stock_\" + str(j) + \"_train.parquet\"\n",
    "        book_train_stock_id_j = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        target_train = pd.read_parquet(\"target_data/target_train.parquet\")\n",
    "        #Filter out the stock we are interested in\n",
    "        target_train_stock_id_j = target_train[target_train[\"stock_id\"] == j]\n",
    "\n",
    "        #getting wap\n",
    "        # Compute the first weighted averaged price for each seconds_in_bucket and time ID \n",
    "        denom1 = book_train_stock_id_j[\"ask_size1\"] + book_train_stock_id_j[\"bid_size1\"]\n",
    "        volprice1 = book_train_stock_id_j[\"bid_price1\"] * book_train_stock_id_j[\"ask_size1\"] + book_train_stock_id_j[\"ask_price1\"] * book_train_stock_id_j[\"bid_size1\"]\n",
    "        book_train_stock_id_j[\"wap1\"] = volprice1/denom1\n",
    "\n",
    "        # Compute the second weighted averaged price for each seconds_in_bucket and time ID \n",
    "\n",
    "        denom2 = book_train_stock_id_j[\"ask_size2\"] + book_train_stock_id_j[\"bid_size2\"]\n",
    "        volprice2 = book_train_stock_id_j[\"bid_price2\"] * book_train_stock_id_j[\"ask_size2\"] + book_train_stock_id_j[\"ask_price2\"] * book_train_stock_id_j[\"bid_size2\"]\n",
    "        \n",
    "        book_train_stock_id_j[\"wap2\"] = volprice2/denom2\n",
    "        book_train_stock_id_j[\"avg_wap\"] = (book_train_stock_id_j[\"wap1\"] + book_train_stock_id_j[\"wap2\"])/2\n",
    "\n",
    "        #get the log returns for stock 0 using avg_wap\n",
    "        book_train_stock_id_j.loc[:, 'log_return'] = log_return(book_train_stock_id_j[\"avg_wap\"])\n",
    "        #just extract all rows that aren't null...\n",
    "        test = book_train_stock_id_j\n",
    "        book_train_stock_id_j = book_train_stock_id_j[~book_train_stock_id_j['log_return'].isnull()]\n",
    "        \n",
    "        #Is previous 10min volatility related to next 10 min vol???!!!\n",
    "        book_train_stock_id_j = book_train_stock_id_j[book_train_stock_id_j[\"seconds_in_bucket\"] >= subset] #Only consider seconds_in_bucket > subset \n",
    "        \n",
    "        realized_vol_id_j = book_train_stock_id_j.groupby(\"time_id\")[\"log_return\"].apply(realized_volatility) #going through book data of stock 0, grouping by time_id and getting realized volatility using log returns-\n",
    "\n",
    "        vol_id_j_df = pd.DataFrame(columns=[target_train_stock_id_j[\"target\"], realized_vol_id_j]) #create a dataframe with target data and realized volatility for each time id\n",
    "        vol_id_j_df = vol_id_j_df.T\n",
    "        vol_id_j_df.reset_index(inplace=True)\n",
    "        vol_id_j_df = vol_id_j_df.rename(columns={\"target\": \"Target Volatility\", \"log_return\": \"Realized Volatility\"})\n",
    "        vol_id_j_df\n",
    "\n",
    "\n",
    "        #get correlation of realized volatility of current time id with subsequent 10 min target volatility.  \n",
    "        realized_t_corr = vol_id_j_df.corr().iloc[0,1]\n",
    "        correlation_dict[\"realized_t_corr\"].append(realized_t_corr)\n",
    "\n",
    "\n",
    "        #Creating a copy of realized-volatiliy~target so we can forward shift 5 times and get the new correlation of every shift...\n",
    "        vol_id_j_df_shifted = vol_id_j_df.copy()\n",
    "        #print(j)\n",
    "        for i in range(1,6):\n",
    "            realized_t_i_corr = \"realized_t_\" + str(i) + \"_corr\"\n",
    "            vol_id_j_df_shifted[\"Realized Volatility\"] = vol_id_j_df_shifted[\"Realized Volatility\"].shift(1)\n",
    "            realized_t_i_corr_value = vol_id_j_df_shifted.corr().iloc[0,1]\n",
    "            correlation_dict[realized_t_i_corr].append(realized_t_i_corr_value)\n",
    "\n",
    "        correlation_dict[\"stock_id\"].append(j)\n",
    "\n",
    "corrHeatmap(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global correlation_dict\n",
    "average_correlation = {key: np.mean(value) for key,value in correlation_dict.items()}\n",
    "average_correlation = pd.DataFrame(list(average_correlation.items()))\n",
    "average_correlation = average_correlation.drop([0], axis=0)\n",
    "col_names = average_correlation.iloc[:, 0]\n",
    "average_correlation = average_correlation.drop([0], axis=1)\n",
    "average_correlation.columns = ['Target']\n",
    "average_correlation = average_correlation.set_index(col_names)\n",
    "average_correlation.index.name = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(10,10), dpi=200)\n",
    "sns.heatmap(data=average_correlation.T,annot=True,square=True, fmt='f')\n",
    "#ax.invert_yaxis()\n",
    "fig.suptitle('Average Correlation between Realized Volatility (Last 10 min) and Target Volatility', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wap(df):\n",
    "        return (df['bid_price1'] * df['ask_size1'] +\n",
    "                df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "\n",
    "def wap2(df):\n",
    "    return (df['bid_price2'] * df['ask_size2'] +\n",
    "            df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "\n",
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "def realized_volatility(returns):\n",
    "    return np.sqrt(np.sum(returns ** 2))\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "def isValidStock(i):\n",
    "    filename = \"stock_trade_train/stock_\" + str(i) + \"_train.parquet\"\n",
    "    print(filename)\n",
    "    if not os.path.exists(filename):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_predictors(stock_id, train_or_test):\n",
    "    stock_data = pd.read_parquet('stock_book_' + train_or_test + '/stock_' + str(stock_id) + '_' + train_or_test + '.parquet')\n",
    "    stock_data = stock_data[stock_data[\"seconds_in_bucket\"] >= 300] \n",
    "    stock_data['avg_wap']= (wap(stock_data)+ wap2(stock_data))/2\n",
    "    stock_data['log_return'] = stock_data.groupby('time_id')['avg_wap'].apply(log_return)\n",
    "\n",
    "    create_feature_dict = {\n",
    "            'log_return':[realized_volatility],\n",
    "    }\n",
    "\n",
    "    result = pd.DataFrame(stock_data.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n",
    "    result.columns = result.columns.map('_'.join).str.strip('_')\n",
    "    return result\n",
    "\n",
    "\n",
    "def target(stock_id, train_or_test):\n",
    "    result = pd.read_parquet('target_data/target_' + train_or_test + '.parquet')\n",
    "    result = result.loc[result['stock_id'] == stock_id]\n",
    "    result = result.drop(['stock_id'], axis = 1)\n",
    "    return result\n",
    "\n",
    "def generate_data(stock_id, train_or_test):\n",
    "    result = pd.merge(target(stock_id, train_or_test), book_predictors(stock_id, train_or_test), on='time_id', how='left')\n",
    "    return result\n",
    "\n",
    "def generate_train_and_test(stock_id):\n",
    "    train = generate_data(stock_id, 'train')\n",
    "    test = generate_data(stock_id, 'test')\n",
    "\n",
    "    X_train = train.drop(['target', 'time_id'], axis = 1)\n",
    "    X_test = test.drop(['target', 'time_id'], axis = 1)\n",
    "\n",
    "    y_train = train['target']\n",
    "    y_test = test['target']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mspe(y_true, y_pred):\n",
    "    return  (np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "total_size = 0\n",
    "total = 0\n",
    "score_list = []\n",
    "\n",
    "for i in range(127):\n",
    "    if not isValidStock(i):\n",
    "        continue\n",
    "\n",
    "    #print(i)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = generate_train_and_test(i)\n",
    "\n",
    "    reg = LinearRegression().fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    total += mspe(y_test, y_pred) * X_test.shape[0]\n",
    "    total_size += X_test.shape[0]\n",
    "    R_2 = reg.score(X_train, y_train)\n",
    "    score_list.append(R_2)\n",
    "\n",
    "\n",
    "\n",
    "total_RMSPE = np.sqrt(total / total_size)\n",
    "total_RMSPE\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_train_and_test(1)\n",
    "X_train\n",
    "#X_train[X_train.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = np.mean(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Performance of the naive prediction: R2 score: {round(R2,2)}, RMSPE: {round(total_RMSPE,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(vol_id_0_df_shifted, x='Realized Volatility', y='Target Volatility', title='Target Volatility randomized vs Realized Volatility Last 10 min')\n",
    "fig.update_yaxes(nticks=20)\n",
    "fig.update_xaxes(nticks=20)\n",
    "fig.update_layout(xaxis_range=[0, 0.03])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_id_0_df_shifted.corr().iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_id_0_df_shifted = vol_id_0_df.copy()\n",
    "vol_id_0_df_shifted[\"Realized Volatility\"] = vol_id_0_df_shifted[\"Realized Volatility\"].shift(1)\n",
    "vol_id_0_df_shifted.corr()\n",
    "#Clearly future time series data has no correlation with previous time series data... Only the prveious 10 min and next 10 min are correlaated. Past data has no effect on future "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_train_stock_id_0[book_train_stock_id_0[\"seconds_in_bucket\"] >= 300]\n",
    "target_train_stock_id_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if previous 5 min vol is better correlated to 10min vol!!!\n",
    "#Is previous 10min volatility related to next 10 min vol???!!!\n",
    "\n",
    "book_train_stock_id_0_5min = book_train_stock_id_0[book_train_stock_id_0[\"seconds_in_bucket\"] >= 300]\n",
    "realized_vol_id_0_5min = book_train_stock_id_0_5min.groupby(\"time_id\")[\"log_return\"].apply(realized_volatility)\n",
    "realized_vol_id_0_5min\n",
    "vol_id_0_df_5min = pd.DataFrame(columns=[target_train_stock_id_0[\"target\"], realized_vol_id_0_5min])\n",
    "realized_vol_id_0_5min\n",
    "#target_train_stock_id_0[\"target\"]\n",
    "vol_id_0_df_5min = vol_id_0_df_5min.T\n",
    "vol_id_0_df_5min\n",
    "vol_id_0_df_5min.reset_index(inplace=True)\n",
    "vol_id_0_df_5min = vol_id_0_df_5min.rename(columns={\"target\": \"Target Volatility\", \"log_return\": \"Realized Volatility Last 5 min\"})\n",
    "vol_id_0_df_5min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(vol_id_0_df_5min, x='Realized Volatility Last 5 min', y='Target Volatility', title='Target Volatility vs Realized Volatility Last 5 min')\n",
    "fig.update_yaxes(nticks=20)\n",
    "fig.update_xaxes(nticks=20)\n",
    "fig.update_layout(xaxis_range=[0, 0.03])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_id_0_df_5min.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if previous 3 min vol is better correlated to 10min vol!!!\n",
    "#Is previous 10min volatility related to next 10 min vol???!!!\n",
    "\n",
    "book_train_stock_id_0_3min = book_train_stock_id_0[book_train_stock_id_0[\"seconds_in_bucket\"] >= 300]\n",
    "realized_vol_id_0_3min = book_train_stock_id_0_3min.groupby(\"time_id\")[\"log_return\"].apply(realized_volatility)\n",
    "realized_vol_id_0_3min\n",
    "vol_id_0_df_3min = pd.DataFrame(columns=[target_train_stock_id_0[\"target\"], realized_vol_id_0_3min])\n",
    "realized_vol_id_0_3min\n",
    "#target_train_stock_id_0[\"target\"]\n",
    "vol_id_0_df_3min = vol_id_0_df_3min.T\n",
    "vol_id_0_df_3min\n",
    "vol_id_0_df_3min.reset_index(inplace=True)\n",
    "vol_id_0_df_3min = vol_id_0_df_3min.rename(columns={\"target\": \"Target Volatility\", \"log_return\": \"Realized Volatility Last 3 min\"})\n",
    "vol_id_0_df_3min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(vol_id_0_df_5min, x='Realized Volatility Last 5 min', y='Target Volatility', title='Target Volatility vs Realized Volatility Last 3 min')\n",
    "fig.update_yaxes(nticks=20)\n",
    "fig.update_xaxes(nticks=20)\n",
    "fig.update_layout(xaxis_range=[0, 0.03])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_id_0_df_3min.corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d41f633da6a0ed462606ff387ac8d93faacd97ec55d72fd1ffe37702897b9b90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
