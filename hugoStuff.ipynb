{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import lightgbm as lgbm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wap(df):\n",
    "    return (df['bid_price1'] * df['ask_size1'] +\n",
    "            df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "\n",
    "def wap2(df):\n",
    "    return (df['bid_price2'] * df['ask_size2'] +\n",
    "            df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "\n",
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "def realized_volatility(returns):\n",
    "    return np.sqrt(np.sum(returns ** 2))\n",
    "\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_predictors(stock_id, train_or_test):\n",
    "    stock_data = pd.read_parquet('stock_book_' + train_or_test + '/stock_' + str(stock_id) + '_' + train_or_test + '.parquet')\n",
    "    stock_data['wap'] = wap(stock_data)\n",
    "    stock_data['log_return'] = stock_data.groupby('time_id')['wap'].apply(log_return)\n",
    "    stock_data['wap2'] = wap2(stock_data)\n",
    "    stock_data['log_return2'] = stock_data.groupby('time_id')['wap2'].apply(log_return)\n",
    "    stock_data['wap_offset'] = abs(stock_data['wap'] - stock_data['wap2'])\n",
    "    stock_data['price_spread'] = (stock_data['ask_price1'] - stock_data['bid_price1']) / ((stock_data['ask_price1'] + stock_data['bid_price1']) / 2)\n",
    "    stock_data['bid_spread'] = stock_data['bid_price1'] - stock_data['bid_price2']\n",
    "    stock_data['ask_spread'] = stock_data['ask_price1'] - stock_data['ask_price2']\n",
    "    stock_data['total_volume'] = (stock_data['ask_size1'] + stock_data['ask_size2']) + (stock_data['bid_size1'] + stock_data['bid_size2'])\n",
    "    stock_data['volume_imbalance'] = abs((stock_data['ask_size1'] + stock_data['ask_size2']) - (stock_data['bid_size1'] + stock_data['bid_size2']))\n",
    "\n",
    "    create_feature_dict = {\n",
    "            'log_return':[realized_volatility],\n",
    "            'log_return2':[realized_volatility],\n",
    "            'wap_offset':[np.mean],\n",
    "            'price_spread':[np.mean],\n",
    "            'bid_spread':[np.mean],\n",
    "            'ask_spread':[np.mean],\n",
    "            'volume_imbalance':[np.mean],\n",
    "            'total_volume':[np.mean],\n",
    "            'wap':[np.mean],\n",
    "                }\n",
    "\n",
    "    return pd.DataFrame(stock_data.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n",
    "\n",
    "def trade_predictors(stock_id, train_or_test):\n",
    "    stock_data = pd.read_parquet('stock_trade_' + train_or_test + '/stock_' + str(stock_id) + '_' + train_or_test + '.parquet')\n",
    "\n",
    "    stock_data['log_return'] = stock_data.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    \n",
    "    aggregate_dictionary = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.mean],\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(stock_data.groupby('time_id').agg(aggregate_dictionary)).reset_index()\n",
    "\n",
    "def target(stock_id, train_or_test):\n",
    "    result = pd.read_parquet('target_data/target_' + train_or_test + '.parquet')\n",
    "    result = result.loc[result['stock_id'] == stock_id]\n",
    "    result = result.drop(['stock_id'], axis = 1)\n",
    "    return result\n",
    "\n",
    "def generate_data(stock_id, train_or_test):\n",
    "    result = pd.merge(target(stock_id, train_or_test), book_predictors(stock_id, train_or_test), on='time_id', how='left')\n",
    "    result = pd.merge(result, trade_predictors(stock_id, train_or_test), on='time_id', how='left')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0 = generate_data(0, 'train')\n",
    "test_0 = generate_data(0, 'test')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4024e72cf1fc492cf2406277bf4e6f7beb5394d9801707712d5b633e8054c31c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
