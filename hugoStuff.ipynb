{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import lightgbm as lgbm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_predictors(stock_id, train_or_test):\n",
    "    stock_data = pd.read_parquet('stock_book_' + train_or_test + '/stock_' + str(stock_id) + '_' + train_or_test + '.parquet')\n",
    "    stock_data['wap'] = wap(stock_data)\n",
    "    stock_data['log_return'] = stock_data.groupby('time_id')['wap'].apply(log_return)\n",
    "    stock_data['wap2'] = wap2(stock_data)\n",
    "    stock_data['log_return2'] = stock_data.groupby('time_id')['wap2'].apply(log_return)\n",
    "    stock_data['wap_offset'] = abs(stock_data['wap'] - stock_data['wap2'])\n",
    "    stock_data['price_spread'] = (stock_data['ask_price1'] - stock_data['bid_price1']) / ((stock_data['ask_price1'] + stock_data['bid_price1']) / 2)\n",
    "    stock_data['bid_spread'] = stock_data['bid_price1'] - stock_data['bid_price2']\n",
    "    stock_data['ask_spread'] = stock_data['ask_price1'] - stock_data['ask_price2']\n",
    "    stock_data['total_volume'] = (stock_data['ask_size1'] + stock_data['ask_size2']) + (stock_data['bid_size1'] + stock_data['bid_size2'])\n",
    "    stock_data['volume_imbalance'] = abs((stock_data['ask_size1'] + stock_data['ask_size2']) - (stock_data['bid_size1'] + stock_data['bid_size2']))\n",
    "\n",
    "    create_feature_dict = {\n",
    "            'log_return':[realized_volatility],\n",
    "            'log_return2':[realized_volatility],\n",
    "            'wap_offset':[np.mean],\n",
    "            'price_spread':[np.mean],\n",
    "            'bid_spread':[np.mean],\n",
    "            'ask_spread':[np.mean],\n",
    "            'volume_imbalance':[np.mean],\n",
    "            'total_volume':[np.mean],\n",
    "            'wap':[np.mean],\n",
    "    }\n",
    "\n",
    "    result = pd.DataFrame(stock_data.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n",
    "    result.columns = result.columns.map('_'.join).str.strip('_')\n",
    "    return result\n",
    "\n",
    "def trade_predictors(stock_id, train_or_test):\n",
    "    stock_data = pd.read_parquet('stock_trade_' + train_or_test + '/stock_' + str(stock_id) + '_' + train_or_test + '.parquet')\n",
    "\n",
    "    stock_data['log_return'] = stock_data.groupby('time_id')['price'].apply(log_return)\n",
    "    \n",
    "    \n",
    "    aggregate_dictionary = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.mean],\n",
    "    }\n",
    "\n",
    "    result = pd.DataFrame(stock_data.groupby('time_id').agg(aggregate_dictionary)).reset_index()\n",
    "    result.columns = result.columns.map('_'.join).str.strip('_')\n",
    "    return result\n",
    "\n",
    "def target(stock_id, train_or_test):\n",
    "    result = pd.read_parquet('target_data/target_' + train_or_test + '.parquet')\n",
    "    result = result.loc[result['stock_id'] == stock_id]\n",
    "    result = result.drop(['stock_id'], axis = 1)\n",
    "    return result\n",
    "\n",
    "def generate_data(stock_id, train_or_test):\n",
    "    result = pd.merge(target(stock_id, train_or_test), book_predictors(stock_id, train_or_test), on='time_id', how='left')\n",
    "    result = pd.merge(result, trade_predictors(stock_id, train_or_test), on='time_id', how='left')\n",
    "    return result\n",
    "\n",
    "def generate_train_and_test(stock_id):\n",
    "    train = generate_data(stock_id, 'train')\n",
    "    test = generate_data(stock_id, 'test')\n",
    "\n",
    "    X_train = train.drop(['target', 'time_id'], axis = 1)\n",
    "    X_test = test.drop(['target', 'time_id'], axis = 1)\n",
    "\n",
    "    y_train = train['target']\n",
    "    y_test = test['target']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_importance(model, feature_names=None, importance_type='gain'):\n",
    "    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n",
    "                                 index=feature_names,\n",
    "                                 columns=['importance']).sort_values('importance')\n",
    "    return importance_df\n",
    "\n",
    "def plot_importance(importance_df, title='',\n",
    "                    save_filepath=None, figsize=(8, 12)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    importance_df.plot.barh(ax=ax)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_filepath is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(save_filepath)\n",
    "    plt.close()\n",
    "\n",
    "def RMSPEMetric():\n",
    "\n",
    "    def RMSPE(yhat, dtrain):\n",
    "        y = dtrain.get_label()\n",
    "        elements = ((y - yhat) / y) ** 2\n",
    "        return 'RMSPE', float(np.sqrt(np.sum(elements) / len(y))), False\n",
    "\n",
    "    return RMSPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 3065, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.004379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's l2: 7.148e-07\ttraining's RMSPE: 0.204012\tvalid_1's l2: 1.14453e-06\tvalid_1's RMSPE: 0.242739\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000726 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 3065, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.004379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's l2: 7.148e-07\ttraining's RMSPE: 0.204012\tvalid_1's l2: 1.14453e-06\tvalid_1's RMSPE: 0.242739\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 3065, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.004379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's l2: 7.148e-07\ttraining's RMSPE: 0.204012\tvalid_1's l2: 1.14453e-06\tvalid_1's RMSPE: 0.242739\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000244 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 3065, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.004379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's l2: 7.148e-07\ttraining's RMSPE: 0.204012\tvalid_1's l2: 1.14453e-06\tvalid_1's RMSPE: 0.242739\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 3065, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.004379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's l2: 7.148e-07\ttraining's RMSPE: 0.204012\tvalid_1's l2: 1.14453e-06\tvalid_1's RMSPE: 0.242739\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 3065, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.004379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's l2: 7.148e-07\ttraining's RMSPE: 0.204012\tvalid_1's l2: 1.14453e-06\tvalid_1's RMSPE: 0.242739\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 3065, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.004379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's l2: 7.148e-07\ttraining's RMSPE: 0.204012\tvalid_1's l2: 1.14453e-06\tvalid_1's RMSPE: 0.242739\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000479 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 3065, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.004379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's l2: 7.148e-07\ttraining's RMSPE: 0.204012\tvalid_1's l2: 1.14453e-06\tvalid_1's RMSPE: 0.242739\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 3065, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.004379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's l2: 7.148e-07\ttraining's RMSPE: 0.204012\tvalid_1's l2: 1.14453e-06\tvalid_1's RMSPE: 0.242739\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 3065, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.004379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's l2: 7.148e-07\ttraining's RMSPE: 0.204012\tvalid_1's l2: 1.14453e-06\tvalid_1's RMSPE: 0.242739\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 3065, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.004379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's l2: 7.148e-07\ttraining's RMSPE: 0.204012\tvalid_1's l2: 1.14453e-06\tvalid_1's RMSPE: 0.242739\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 3065, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.004379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's l2: 7.148e-07\ttraining's RMSPE: 0.204012\tvalid_1's l2: 1.14453e-06\tvalid_1's RMSPE: 0.242739\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000404 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3242\n",
      "[LightGBM] [Info] Number of data points in the train set: 3065, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 0.004379\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[39]\ttraining's l2: 7.148e-07\ttraining's RMSPE: 0.204012\tvalid_1's l2: 1.14453e-06\tvalid_1's RMSPE: 0.242739\n",
      "14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hugog\\OneDrive - UNSW\\COMP9417\\9417optivized\\hugoStuff.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=5'>6</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(i)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=9'>10</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m generate_train_and_test(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=11'>12</a>\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=12'>13</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mn_jobs\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m8\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=13'>14</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mRMSE\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=14'>15</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=16'>17</a>\u001b[0m train \u001b[39m=\u001b[39m lgbm\u001b[39m.\u001b[39mDataset(X_train, label\u001b[39m=\u001b[39my_train)\n",
      "\u001b[1;32mc:\\Users\\hugog\\OneDrive - UNSW\\COMP9417\\9417optivized\\hugoStuff.ipynb Cell 8\u001b[0m in \u001b[0;36mgenerate_train_and_test\u001b[1;34m(stock_id)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=57'>58</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_train_and_test\u001b[39m(stock_id):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=58'>59</a>\u001b[0m     train \u001b[39m=\u001b[39m generate_data(stock_id, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=59'>60</a>\u001b[0m     test \u001b[39m=\u001b[39m generate_data(stock_id, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=61'>62</a>\u001b[0m     X_train \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtime_id\u001b[39m\u001b[39m'\u001b[39m], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\hugog\\OneDrive - UNSW\\COMP9417\\9417optivized\\hugoStuff.ipynb Cell 8\u001b[0m in \u001b[0;36mgenerate_data\u001b[1;34m(stock_id, train_or_test)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=52'>53</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_data\u001b[39m(stock_id, train_or_test):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=53'>54</a>\u001b[0m     result \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(target(stock_id, train_or_test), book_predictors(stock_id, train_or_test), on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtime_id\u001b[39m\u001b[39m'\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=54'>55</a>\u001b[0m     result \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(result, trade_predictors(stock_id, train_or_test), on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtime_id\u001b[39m\u001b[39m'\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=55'>56</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;32mc:\\Users\\hugog\\OneDrive - UNSW\\COMP9417\\9417optivized\\hugoStuff.ipynb Cell 8\u001b[0m in \u001b[0;36mbook_predictors\u001b[1;34m(stock_id, train_or_test)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=1'>2</a>\u001b[0m stock_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(\u001b[39m'\u001b[39m\u001b[39mstock_book_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m train_or_test \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/stock_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(stock_id) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m train_or_test \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.parquet\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=2'>3</a>\u001b[0m stock_data[\u001b[39m'\u001b[39m\u001b[39mwap\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m wap(stock_data)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=3'>4</a>\u001b[0m stock_data[\u001b[39m'\u001b[39m\u001b[39mlog_return\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m stock_data\u001b[39m.\u001b[39;49mgroupby(\u001b[39m'\u001b[39;49m\u001b[39mtime_id\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m'\u001b[39;49m\u001b[39mwap\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(log_return)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=4'>5</a>\u001b[0m stock_data[\u001b[39m'\u001b[39m\u001b[39mwap2\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m wap2(stock_data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hugog/OneDrive%20-%20UNSW/COMP9417/9417optivized/hugoStuff.ipynb#ch0000008?line=5'>6</a>\u001b[0m stock_data[\u001b[39m'\u001b[39m\u001b[39mlog_return2\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m stock_data\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mtime_id\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mwap2\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(log_return)\n",
      "File \u001b[1;32mc:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:244\u001b[0m, in \u001b[0;36mSeriesGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[39m@Appender\u001b[39m(\n\u001b[0;32m    239\u001b[0m     _apply_docs[\u001b[39m\"\u001b[39m\u001b[39mtemplate\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    240\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mseries\u001b[39m\u001b[39m\"\u001b[39m, examples\u001b[39m=\u001b[39m_apply_docs[\u001b[39m\"\u001b[39m\u001b[39mseries_examples\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    241\u001b[0m     )\n\u001b[0;32m    242\u001b[0m )\n\u001b[0;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mapply(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1414\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1412\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1413\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1414\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_apply_general(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selected_obj)\n\u001b[0;32m   1415\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   1416\u001b[0m         \u001b[39m# gh-20949\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m         \u001b[39m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1421\u001b[0m         \u001b[39m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m         \u001b[39m# on a string grouper column\u001b[39;00m\n\u001b[0;32m   1424\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_group_selection_context():\n",
      "File \u001b[1;32mc:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1460\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same)\u001b[0m\n\u001b[0;32m   1457\u001b[0m \u001b[39mif\u001b[39;00m not_indexed_same \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1458\u001b[0m     not_indexed_same \u001b[39m=\u001b[39m mutated \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmutated\n\u001b[1;32m-> 1460\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_applied_output(\n\u001b[0;32m   1461\u001b[0m     data, values, not_indexed_same\u001b[39m=\u001b[39;49mnot_indexed_same\n\u001b[0;32m   1462\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:398\u001b[0m, in \u001b[0;36mSeriesGroupBy._wrap_applied_output\u001b[1;34m(self, data, values, not_indexed_same)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[39mreturn\u001b[39;00m res_ser\n\u001b[0;32m    397\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(values[\u001b[39m0\u001b[39m], (Series, DataFrame)):\n\u001b[1;32m--> 398\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_concat_objects(values, not_indexed_same\u001b[39m=\u001b[39;49mnot_indexed_same)\n\u001b[0;32m    399\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    400\u001b[0m     \u001b[39m# GH #6265 #24880\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_constructor(\n\u001b[0;32m    402\u001b[0m         data\u001b[39m=\u001b[39mvalues, index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouper\u001b[39m.\u001b[39mresult_index, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39mname\n\u001b[0;32m    403\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hugog\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1041\u001b[0m, in \u001b[0;36mGroupBy._concat_objects\u001b[1;34m(self, values, not_indexed_same)\u001b[0m\n\u001b[0;32m   1034\u001b[0m     ax \u001b[39m=\u001b[39m ax[mask]\n\u001b[0;32m   1036\u001b[0m \u001b[39m# this is a very unfortunate situation\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[39m# we can't use reindex to restore the original order\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m# when the ax has duplicates\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[39m# so we resort to this\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[39m# GH 14776, 30667\u001b[39;00m\n\u001b[1;32m-> 1041\u001b[0m \u001b[39mif\u001b[39;00m ax\u001b[39m.\u001b[39mhas_duplicates \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39maxes[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis]\u001b[39m.\u001b[39mequals(ax):\n\u001b[0;32m   1042\u001b[0m     indexer, _ \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mget_indexer_non_unique(ax\u001b[39m.\u001b[39m_values)\n\u001b[0;32m   1043\u001b[0m     indexer \u001b[39m=\u001b[39m algorithms\u001b[39m.\u001b[39munique1d(indexer)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "total_size = 0\n",
    "\n",
    "for i in range(127):    \n",
    "    if not isValidStock(i):\n",
    "        continue\n",
    "\n",
    "    print(i)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = generate_train_and_test(1)\n",
    "\n",
    "    params = {\n",
    "        'n_jobs': 8,\n",
    "        'objective': 'RMSE',\n",
    "    }\n",
    "\n",
    "    train = lgbm.Dataset(X_train, label=y_train)\n",
    "    test = lgbm.Dataset(X_test, label=y_test)\n",
    "\n",
    "    clf = lgbm.train(params, train, 1000, valid_sets=[train, test], feval=RMSPEMetric(), early_stopping_rounds=100, verbose_eval = -1)\n",
    "\n",
    "    total += clf.best_score.get('valid_1')['RMSPE'] * X_test.shape[0]\n",
    "    total_size += X_test.shape[0]\n",
    "\n",
    "total_RSMPE = total / total_size\n",
    "total_RSMPE\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4024e72cf1fc492cf2406277bf4e6f7beb5394d9801707712d5b633e8054c31c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
